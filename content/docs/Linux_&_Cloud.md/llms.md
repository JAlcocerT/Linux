---
title: LLMs
type: docs
prev: /docs/gcp/
next: docs/ansible/
draft: false
---


Generative AI and particularly LLMs are taking over.

You can **get LLMs running** in your personal computer or in big servers just for you or whoever you want to give access.

## Ollama

Probably the quickest way to get LLMs working, specially if you already have Docker installed.

* <https://fossengineer.com/selfhosting-llms-ollama/>

## PrivateGPT

* <https://fossengineer.com/selfhosting-local-llms-with-privateGPT/>

## TextGeneration Web UI

* <https://fossengineer.com/Generative-AI-LLMs-locally-with-cpu/>

<iframe width="560" height="315" src="https://www.youtube.com/embed/Ib3nQu5bB_k" frameborder="0" allowfullscreen></iframe>

## FAQ

### How to monitor Hardware while using LLMs?

You can [setup Netdata with Docker](https://fossengineer.com/selfhosting-server-monitoring-with-netdata-and-docker/) really quick.

It will give you insights on workload, temperatures of your Hardware where you run LLMs.