---
title: LLMs
type: docs
prev: /docs/gcp/
next: docs/ansible/
draft: false
---


Generative AI and particularly LLMs are taking over.

You can **get LLMs running** in your personal computer or in big servers just for you or whoever you want to give access.

## Ollama

Probably the quickest way to get LLMs working, specially if you already have Docker installed.

* <https://fossengineer.com/selfhosting-llms-ollama/>

<iframe width="560" height="315" src="https://www.youtube.com/embed/jl9bfPsBBcM" frameborder="0" allowfullscreen></iframe>

## PrivateGPT

* <https://fossengineer.com/selfhosting-local-llms-with-privateGPT/>

<iframe width="560" height="315" src="https://www.youtube.com/embed/Ib3nQu5bB_k" frameborder="0" allowfullscreen></iframe>

## TextGeneration Web UI

* <https://fossengineer.com/Generative-AI-LLMs-locally-with-cpu/>

<iframe width="560" height="315" src="https://www.youtube.com/embed/-zNWDTqKF1E" frameborder="0" allowfullscreen></iframe>

## FAQ

### How to monitor Hardware while using LLMs?

You can [setup Netdata with Docker](https://fossengineer.com/selfhosting-server-monitoring-with-netdata-and-docker/) really quick.

It will give you insights on workload, temperatures of your Hardware where you run LLMs.

### Can I use LLMs to Code?

Yes, there are many ways to replace Github Copilot for Free:

* [Tabby](https://fossengineer.com/selfhosting-Tabby-coding-assistant/)
* [LLama Coder](https://github.com/ex3ndr/llama-coder) in a [vscode extension](https://marketplace.visualstudio.com/items?itemName=ex3ndr.llama-coder&ssr=false#review-details)

### And to Power my Notes?

* LogSeq + Ollama: <https://github.com/omagdy7/ollama-logseq>